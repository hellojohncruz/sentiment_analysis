---
title: "Text Mining and Sentiment Analysis"
author: "John Cruz"
date: "2023-04-01"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Introduction

Text Mining with R, [Chapter 2](https://www.tidytextmining.com/sentiment.html), looks at sentiment analysis. The authors provide an example using the text of Jane Austen’s six completed, published novels from the *janeaustenr* library. All the code is originally credited to the authors, unless otherwise noted.

---

## Required Libraries

```{r library, message=FALSE}
library(tidyverse)
library(tidytext)
library(janeaustenr)
library(stringr)
library(jsonlite)
library(glue)
library(lubridate)
```

---

## Tidy Up Jane Austen's Work

The authors take the text of the novels and converts the text to the tidy format using *unnest_tokens()*. They also create other columns to keep track of which line and chapter of the book each word comes from. 

```{r example-1}
tidy_books <- 
  austen_books() |> 
  group_by(book) |> 
  mutate(linenumber = row_number(),
         chapter = cumsum(str_detect(text, regex("^chapter [\\divxlc]", ignore_case = TRUE)))) |> 
  ungroup() |> 
  unnest_tokens(word, text)

knitr::kable(head(tidy_books), caption = "Brief View of Tokenized Words")
```

## Determining Overall Sentiment

Next, count up how many positive and negative words there are in defined sections of each book, along with a net sentiment score. They define an index here to keep track of where they are in the narrative. The index counts up sections of 80 lines of text.

```{r example-2, message=FALSE, warning=FALSE}
jane_austen_sentiment <- 
  tidy_books |>
  inner_join(get_sentiments("bing")) |>
  count(book, index = linenumber %/% 80, sentiment) |>
  pivot_wider(names_from = sentiment, values_from = n, values_fill = 0) |> 
  mutate(sentiment = positive - negative)

knitr::kable(head(jane_austen_sentiment), caption = "Brief View of Sentiment Scores by Indexing")
```

## Visualizing Sentiment throughout each Novel

Finally, plot how each novel changes toward more positive or negative sentiment over the trajectory of the story.

```{r example-3}
jane_austen_sentiment |> 
  ggplot(aes(index, sentiment, fill = book)) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~book, ncol = 2, scales = "free_x")
```

## Which Chapter has the Most Negative Words?

The authors also provide a proportion table to determine which chapter has the most negative words in each book. 

```{r example-4, warning=FALSE, message=FALSE}
bingnegative <- 
  get_sentiments("bing") |> 
  filter(sentiment == "negative")

wordcounts <- tidy_books |>
  group_by(book, chapter) |>
  summarize(words = n())

ratio_tbl <-
  tidy_books |>
  semi_join(bingnegative) |>
  group_by(book, chapter) |>
  summarize(negativewords = n()) |>
  left_join(wordcounts, by = c("book", "chapter")) |>
  mutate(ratio = negativewords/words) |>
  filter(chapter != 0) |>
  slice_max(ratio, n = 1) |> 
  ungroup()

knitr::kable(ratio_tbl)
```

---

**Note:** All work from this point forward has been created by me. 

## Corpus: NY Times Articles

Lets look at the NY Times published articles in March 2023. However, I will primarily look at the lead paragraph of each article. The goal is to get an idea of what kind of sentiment is being used within the different sections of each paragraph they offer such as Arts, U.S. and Sports. I will also look into seeing if certain times of the day lends itself more to positive or negative sentiments. 

## Connect to NY Times API

```{r nyt-api}
api_cnxn <- 
  fromJSON(glue("https://api.nytimes.com/svc/archive/v1/2023/3.json?api-key={rstudioapi::askForPassword('Enter NY Times API Key')}"), flatten = TRUE)

ny_times <- 
    as.data.frame(api_cnxn) |> 
    janitor::clean_names()

write_csv(ny_times, 'ny_times.csv')
```

## Tidy Up Article Data

```{r update-columns}
section_df <-
  ny_times |> 
  select(response_docs_pub_date, response_docs_section_name, response_docs_lead_paragraph) |> 
  mutate(response_docs_pub_date = str_extract(response_docs_pub_date, "[:graph:]*(?=\\+)")) |> 
  rename(pub_date = response_docs_pub_date, lead_paragraph = response_docs_lead_paragraph, section = response_docs_section_name)

section_df$pub_date <- 
  section_df$pub_date |> 
  ymd_hms()
```

## Tokenize 

```{r example}
tokenize_df <-
  section_df |> 
  unnest_tokens(word, lead_paragraph)

knitr::kable(head(tokenize_df))
```

## Title

```{r example, message=FALSE}
sentiment_df <- 
  tokenize_df |>
  inner_join(get_sentiments("bing")) |>
  count(section, sentiment) |>
  pivot_wider(names_from = sentiment, values_from = n, values_fill = 0) |> 
  mutate(total_words = negative + positive, 
         ovr_sentiment = positive - negative, 
        pct = round(ovr_sentiment/total_words *100, 2))

knitr::kable(sentiment_df, caption = "Rename")
```

## Title

```{r example}
```

## Package Lexicon
hash_sentiment_senticnet

Description
A data.table dataset containing an augmented version of Cambria, Poria, Bajpai,& Schuller’s
(2016) positive/negative word list as sentiment lookup values.

https://cran.r-project.org/web/packages/lexicon/lexicon.pdf

```{r example}

```

## Title

```{r example}
```

## Title

```{r example}
```

## Title

```{r example}
```






