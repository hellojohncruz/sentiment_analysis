---
title: "Text Mining and Sentiment Analysis"
author: "John Cruz"
date: "2023-04-01"
output:
  pdf_document: default
  html_document: default
urlcolor: blue
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Introduction

Text Mining with R, [Chapter 2](https://www.tidytextmining.com/sentiment.html), looks at sentiment analysis. The authors provide an example using the text of Jane Austenâ€™s six completed, published novels from the *janeaustenr* library. All the code is originally credited to the authors, unless otherwise noted.

---

## Required Libraries

```{r library, message=FALSE}
library(tidyverse)
library(tidytext)
library(janeaustenr)
library(stringr)
library(jsonlite)
library(glue)
library(lubridate)
library(ggrepel)
```

---

## Tidy Up Jane Austen's Work

The authors take the text of the novels and converts the text to the tidy format using *unnest_tokens()*. They also create other columns to keep track of which line and chapter of the book each word comes from. 

```{r example-1}
tidy_books <- 
  austen_books() |> 
  group_by(book) |> 
  mutate(linenumber = row_number(),
         chapter = cumsum(str_detect(text, regex("^chapter [\\divxlc]", ignore_case = TRUE)))) |> 
  ungroup() |> 
  unnest_tokens(word, text)

knitr::kable(head(tidy_books), caption = "Brief View of Tokenized Words")
```

## Determining Overall Sentiment

Next, count up how many positive and negative words there are in defined sections of each book, along with a net sentiment score. They define an index here to keep track of where they are in the narrative. The index counts up sections of 80 lines of text.

```{r example-2, message=FALSE, warning=FALSE}
jane_austen_sentiment <- 
  tidy_books |>
  inner_join(get_sentiments("bing")) |>
  count(book, index = linenumber %/% 80, sentiment) |>
  pivot_wider(names_from = sentiment, values_from = n, values_fill = 0) |> 
  mutate(sentiment = positive - negative)

knitr::kable(head(jane_austen_sentiment), caption = "Brief View of Sentiment Scores by Indexing")
```

## Visualizing Sentiment throughout each Novel

Finally, plot how each novel changes toward more positive or negative sentiment over the trajectory of the story.

```{r example-3}
jane_austen_sentiment |> 
  ggplot(aes(index, sentiment, fill = book)) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~book, ncol = 2, scales = "free_x")
```

## Which Chapter has the Most Negative Words?

The authors also provide a proportion table to determine which chapter has the most negative words in each book. 

```{r example-4, warning=FALSE, message=FALSE}
bingnegative <- 
  get_sentiments("bing") |> 
  filter(sentiment == "negative")

wordcounts <- tidy_books |>
  group_by(book, chapter) |>
  summarize(words = n())

ratio_tbl <-
  tidy_books |>
  semi_join(bingnegative) |>
  group_by(book, chapter) |>
  summarize(negativewords = n()) |>
  left_join(wordcounts, by = c("book", "chapter")) |>
  mutate(ratio = negativewords/words) |>
  filter(chapter != 0) |>
  slice_max(ratio, n = 1) |> 
  ungroup()

knitr::kable(ratio_tbl)
```

---

**Note:** All work from this point forward has been created by me. 

## Corpus: NY Times Articles

Lets look at the NY Times published articles in March 2023. However, I will primarily look at the lead paragraph of each article. The goal is to get an idea of what kind of sentiment is being used within the different sections of each paragraph they offer such as Arts, U.S. and Sports. I will also look into seeing if certain times of the day lends itself more to positive or negative sentiments. 

## Connect to NY Times API

```{r nyt-api}
api_cnxn <- 
  fromJSON(glue("https://api.nytimes.com/svc/archive/v1/2023/3.json?api-key={rstudioapi::askForPassword('Enter NY Times API Key')}"), flatten = TRUE)

ny_times <- 
    as.data.frame(api_cnxn) |> 
    janitor::clean_names()

write_csv(ny_times, 'ny_times.csv')
```

## Tidy Up Article Data

Clean up column formatting for published dates.

```{r update-columns}
section_df <-
  ny_times |> 
  select(response_docs_pub_date, response_docs_section_name, response_docs_lead_paragraph) |> 
  mutate(response_docs_pub_date = str_extract(response_docs_pub_date, "[:graph:]*(?=\\+)")) |> 
  rename(pub_date = response_docs_pub_date, lead_paragraph = response_docs_lead_paragraph, section = response_docs_section_name)

section_df$pub_date <- 
  section_df$pub_date |> 
  ymd_hms()

section_df$hour <- 
  section_df$pub_date |> 
  hour()
```

## Tokenize Sections

```{r tokenize}
tokenize_df <-
  section_df |> 
  unnest_tokens(word, lead_paragraph)

knitr::kable(head(tokenize_df))
```

## Categorize Sentiments between Sections

Here, we can see that the most negative overall sentiment is the U.S. section at 71%. The most positive overall sentiment is Arts at 22%. 

```{r grouped-sentiments, message=FALSE, warning=FALSE}
sentiment_df <- 
  tokenize_df |>
  inner_join(get_sentiments("bing")) |>
  count(section, sentiment) |>
  pivot_wider(names_from = sentiment, values_from = n, values_fill = 0) |> 
  mutate(total_words = negative + positive, 
         ovr_sentiment = positive - negative, 
        pct = round(ovr_sentiment/total_words *100, 2))

knitr::kable(sentiment_df, caption = "Overall Sentiment based on Section")

sentiment_df |> 
  ggplot(aes(x = negative, y = positive, label = section)) +
  geom_point() +
  geom_label_repel(box.padding = 0.35) +
  xlim(0, 3000) +
  ylim(0, 3000) 
```

## Categorize Sentiments between Time of Day

When categorizing based on time of day, 0600 hour has the most positive leading paragraph sentiment at 31%, while at 1200 hour, it has the largest difference in negative sentiment at 88%. 

```{r grouped-sentiments-2, message=FALSE}
pub_date_df <- 
  tokenize_df |>
  inner_join(get_sentiments("bing")) |>
  group_by() |> 
  count(hour, sentiment) |>
  pivot_wider(names_from = sentiment, values_from = n, values_fill = 0) |> 
  mutate(total_words = negative + positive, 
         ovr_sentiment = positive - negative, 
        pct = round(ovr_sentiment/total_words *100, 2))

knitr::kable(pub_date_df, caption = "Overall Sentiment based on Hour")

pub_date_df |> 
  ggplot(aes(x = hour, y = pct)) +
  geom_bar(stat = 'identity')
```

## Lexicon Library

Researching for other types of lexicon libraries R has to offer, I came across the **lexicon** package. Here it contains many different methods, one of them which is the *hash_sentiment_senticnet()* function. It is a data.table dataset containing an augmented version of Cambria, Poria, Bajpai,& Schuller's (2016) positive/negative word list as sentiment lookup values. Further documentation can be found [here](https://cran.r-project.org/web/packages/lexicon/lexicon.pdf). 

```{r import-lexicon, message=FALSE}
library(lexicon)

knitr::kable(head(hash_sentiment_senticnet), caption = 'Glance of Lookup Values')
```

## Calculate Sentiment based on New Sentiment Lexicon

What we see is that the slope relationship between positive and negative words are much more favorable for positive words when comparing total word counts to each section. This is because I primitively categorized each word based on either being positive or negative without it's strength value accounted for. However, the U.S. section still shows as the leader for all sections with negative words overall.

```{r lexicon-1}
lexicon_df <-
  hash_sentiment_senticnet |> 
  rename(word = x, value = y)
```

```{r lexicon-section, warning = FALSE, message=FALSE}
lex_sentiment_df <- 
  tokenize_df |>
  inner_join(lexicon_df) 

lex_section_df <-  
  lex_sentiment_df |> 
  mutate(type = case_when(value < 0 ~ "negative",
            value > 0 ~ "positive")) |> 
  group_by(section, type) |> 
  summarise(total_words = n()) |> 
  pivot_wider(names_from = type, values_from = total_words) |> 
  mutate(total_words = negative + positive)

knitr::kable(lex_section_df, caption = "Overall Sentiment based on Section")

lex_section_df |> 
  ggplot(aes(x = negative, y = positive, label = section)) +
  geom_point() +
  geom_label_repel(box.padding = 0.35) +
  xlim(0, 11000) +
  ylim(0, 11000) 
```

## Conclusion

Based on the lexicon sentiment package used, the simple values used can change drastically, however, overall the general tendacies are the same such that the U.S section within in the NY Times generally tends to have more ngative sentiment words being used. 

